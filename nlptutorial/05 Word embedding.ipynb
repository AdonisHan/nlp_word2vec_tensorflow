{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adonishan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# word2vec\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://solarisailab.com/archives/374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]   # header 제외\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-27-df2de4e7855c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-df2de4e7855c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    train_data = read_data('C:\\Users\\Adonishan\\nlp\\rvw.txt')\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data('C:\\Users\\Adonishan\\nlp\\rvw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-77f203de1987>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_docs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "tagger = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_docs = [row[1] for row in train_data]\n",
    "sentences = [tokenize(d) for d in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(*tokenize(u'악당 영웅'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] [b'anarchism', b'originated', b'as', b'a', b'term', b'of', b'abuse', b'first', b'used', b'against']\n",
      "3081 b'originated' -> 12 b'as'\n",
      "3081 b'originated' -> 5234 b'anarchism'\n",
      "12 b'as' -> 3081 b'originated'\n",
      "12 b'as' -> 6 b'a'\n",
      "6 b'a' -> 195 b'term'\n",
      "6 b'a' -> 12 b'as'\n",
      "195 b'term' -> 2 b'of'\n",
      "195 b'term' -> 6 b'a'\n",
      "WARNING:tensorflow:From C:\\Users\\Adonishan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step  0 :  324.18231201171875\n",
      "Nearest to b'three': b'accepting', b'disciplinary', b'hoard', b'maternal', b'lumi', b'philos', b'castille', b'oldham',\n",
      "Nearest to b'united': b'micheal', b'oxidizing', b'dramatized', b'paranthropus', b'blackburn', b'humours', b'lieder', b'airbus',\n",
      "Nearest to b's': b'dealings', b'nand', b'gentiles', b'speculated', b'unomig', b'item', b'massage', b'king',\n",
      "Nearest to b'over': b'langevin', b'acapulco', b'krupp', b'benefited', b'straddles', b'yemenite', b'disputes', b'vashem',\n",
      "Nearest to b'for': b'acrobatics', b'ewell', b'gravestone', b'cm', b'provence', b'rice', b'sunda', b'banna',\n",
      "Nearest to b'its': b'bombed', b'carat', b'confederates', b'fluency', b'exits', b'washing', b'ss', b'landmine',\n",
      "Nearest to b'more': b'predator', b'success', b'resumes', b'shrew', b'searchlight', b'supplanting', b'siwa', b'hemingway',\n",
      "Nearest to b'some': b'signaling', b'diminish', b'astride', b'rastafari', b'vim', b'nisibis', b'autopilot', b'euripides',\n",
      "Nearest to b'so': b'beck', b'deoxyribose', b'proper', b'ezln', b'generalisation', b'brutality', b'hitler', b'tove',\n",
      "Nearest to UNK: b'nuncio', b'expressive', b'misplaced', b'donner', b'foiled', b'apis', b'mahesh', b'conectiva',\n",
      "Nearest to b'one': b'concomitant', b'norte', b'rovers', b'pinkish', b'nihon', b'rigor', b'inoue', b'elia',\n",
      "Nearest to b'states': b'legionnaires', b'heb', b'scala', b'mingus', b'exposing', b'kodak', b'diploma', b'rumours',\n",
      "Nearest to b'may': b'vanuatu', b'sporting', b'davidic', b'nn', b'fool', b'reassigned', b'discrepancy', b'mbe',\n",
      "Nearest to b'this': b'savoie', b'connors', b'aardwolf', b'toilet', b'bernini', b'buttocks', b'diplomat', b'macdonald',\n",
      "Nearest to b'six': b'battleground', b'klimt', b'rolf', b'rouse', b'daemen', b'span', b'his', b'equality',\n",
      "Nearest to b'however': b'montessori', b'rc', b'tristan', b'declared', b'dawn', b'yom', b'excision', b'bracelet',\n",
      "Average loss at step  2000 :  113.66088148880004\n",
      "Average loss at step  4000 :  52.858277815103534\n",
      "Average loss at step  6000 :  33.34128262615204\n",
      "Average loss at step  8000 :  23.840954282164574\n",
      "Average loss at step  10000 :  17.905047545433046\n",
      "Nearest to b'three': b'zero', b'one', b'victoriae', b'altenberg', b'accepting', b'analogue', b'amo', b'nine',\n",
      "Nearest to b'united': b'agave', b'victoriae', b'saints', b'investigation', b'nine', b'in', b'king', b'cc',\n",
      "Nearest to b's': b'and', b'zero', b'gentiles', b'its', b'jacobi', b'in', b'of', b'zorn',\n",
      "Nearest to b'over': b'disputes', b'about', b'victoriae', b'straddles', b'exclude', b'telephone', b'vashem', b'finally',\n",
      "Nearest to b'for': b'in', b'and', b'of', b'at', b'painter', b'altenberg', b'american', b'from',\n",
      "Nearest to b'its': b'the', b'a', b'their', b'airplane', b'vs', b'sanjaks', b'unfortunate', b'reginae',\n",
      "Nearest to b'more': b'success', b'and', b'victoriae', b'aol', b'someone', b'nd', b'mathbf', b'sunflower',\n",
      "Nearest to b'some': b'the', b'android', b'scene', b'czech', b'hieroglyphs', b'boston', b'mathematics', b'compete',\n",
      "Nearest to b'so': b'proper', b'implicit', b'hitler', b'beck', b'till', b'nucleus', UNK, b'victoriae',\n",
      "Nearest to UNK: b'victoriae', b'the', b'and', b'one', b'vs', b'aarhus', b'altenberg', b'mathbf',\n",
      "Nearest to b'one': b'reginae', b'austin', b'gland', b'victoriae', b'aarhus', b'two', b'vs', UNK,\n",
      "Nearest to b'states': b'mathbf', b'ditko', b'heb', b'breeding', b'approximately', b'exposing', b'lowest', b'known',\n",
      "Nearest to b'may': b'nine', b'sporting', b'vanuatu', b'allah', b'paraguay', b'fool', b'gb', b'co',\n",
      "Nearest to b'this': b'a', b'one', b'aardwolf', b'scale', b'the', b'vs', b'awarded', b'it',\n",
      "Nearest to b'six': b'nine', b'zero', b'homomorphism', b'alpina', b'victoriae', b'one', b'two', b'sadler',\n",
      "Nearest to b'however': b'rc', b'dawn', b'agave', b'basins', b'declared', b'download', b'bracelet', b'montessori',\n",
      "Average loss at step  12000 :  13.765966293811799\n",
      "Average loss at step  14000 :  11.714144805788994\n",
      "Average loss at step  16000 :  9.942265109062195\n",
      "Average loss at step  18000 :  8.558620349287986\n",
      "Average loss at step  20000 :  7.752133142828941\n",
      "Nearest to b'three': b'eight', b'two', b'zero', b'nine', b'five', b'six', b'four', b'one',\n",
      "Nearest to b'united': b'agave', b'investigation', b'agouti', b'micheal', b'victoriae', b'lipids', b'airbus', b'cc',\n",
      "Nearest to b's': b'and', b'the', b'zero', b'two', b'gentiles', b'batll', b'phi', b'its',\n",
      "Nearest to b'over': b'about', b'acapulco', b'disputes', b'victoriae', b'in', b'represents', b'straddles', b'efficient',\n",
      "Nearest to b'for': b'in', b'of', b'agouti', b'and', b'at', b'with', b'from', b'altenberg',\n",
      "Nearest to b'its': b'the', b'their', b'his', b'a', b'unfortunate', b's', b'airplane', b'tony',\n",
      "Nearest to b'more': b'success', b'someone', b'predator', b'replica', b'sunflower', b'victoriae', b'oscillator', b'still',\n",
      "Nearest to b'some': b'the', b'several', b'android', b'turin', b'compete', b'gnaeus', b'a', b'scene',\n",
      "Nearest to b'so': b'beck', b'proper', b'implicit', b'hitler', b'acetylene', b'till', b'replica', UNK,\n",
      "Nearest to UNK: b'victoriae', b'vs', b'dasyprocta', b'and', b'two', b'reginae', b'agouti', b'et',\n",
      "Nearest to b'one': b'two', b'three', b'polyhedra', b'reginae', b'eight', b'dasyprocta', b'six', b'four',\n",
      "Nearest to b'states': b'amino', b'mathbf', b'lipids', b'ditko', b'heb', b'breeding', b'the', b'approximately',\n",
      "Nearest to b'may': b'vanuatu', b'paraguay', b'would', b'will', b'sporting', b'agouti', b'nine', b'athena',\n",
      "Nearest to b'this': b'a', b'the', b'it', b'which', b'scale', b'his', b'aardwolf', b'one',\n",
      "Nearest to b'six': b'nine', b'eight', b'zero', b'five', b'two', b'three', b'seven', b'four',\n",
      "Nearest to b'however': b'rc', b'agouti', b'dawn', b'garand', b'download', b'cassettes', b'montessori', b'dasyprocta',\n",
      "Average loss at step  22000 :  7.24419096171856\n",
      "Average loss at step  24000 :  6.93086699450016\n",
      "Average loss at step  26000 :  6.686886855721474\n",
      "Average loss at step  28000 :  6.137970935344696\n",
      "Average loss at step  30000 :  6.1815847988128665\n",
      "Nearest to b'three': b'five', b'eight', b'two', b'six', b'four', b'seven', b'nine', b'zero',\n",
      "Nearest to b'united': b'agave', b'investigation', b'micheal', b'waas', b'agouti', b'airbus', b'following', b'in',\n",
      "Nearest to b's': b'and', b'of', b'zero', b'or', b'two', b'the', b'his', b'batll',\n",
      "Nearest to b'over': b'about', b'disputes', b'acapulco', b'abet', b'in', b'abitibi', b'victoriae', b'represents',\n",
      "Nearest to b'for': b'in', b'agouti', b'of', b'at', b'and', b'with', b'from', b'altenberg',\n",
      "Nearest to b'its': b'their', b'the', b'his', b'a', b'any', b'bos', b'unfortunate', b'vdash',\n",
      "Nearest to b'more': b'success', b'replica', b'thinkpad', b'most', b'victoriae', b'someone', b'oscillator', b'predator',\n",
      "Nearest to b'some': b'several', b'the', b'many', b'android', b'signaling', b'compete', b'akita', b'machines',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'implicit', b'acetylene', b'and', b'till', b'agouti',\n",
      "Nearest to UNK: b'victoriae', b'reuptake', b'dasyprocta', b'akita', b'agouti', b'abitibi', b'et', b'vs',\n",
      "Nearest to b'one': b'two', b'three', b'four', b'six', b'eight', b'seven', b'five', b'reginae',\n",
      "Nearest to b'states': b'aediles', b'ditko', b'amino', b'mathbf', b'lipids', b'heb', b'breeding', b'exposing',\n",
      "Nearest to b'may': b'would', b'will', b'can', b'paraguay', b'vanuatu', b'nine', b'eight', b'sporting',\n",
      "Nearest to b'this': b'it', b'which', b'the', b'a', b'reuptake', b'akita', b'amalthea', b'that',\n",
      "Nearest to b'six': b'eight', b'nine', b'five', b'four', b'seven', b'three', b'zero', b'two',\n",
      "Nearest to b'however': b'rc', b'dawn', b'dasyprocta', b'agouti', b'download', b'garand', b'is', b'agave',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  32000 :  5.908186203241348\n",
      "Average loss at step  34000 :  5.831139935016632\n",
      "Average loss at step  36000 :  5.682317431330681\n",
      "Average loss at step  38000 :  5.25980399274826\n",
      "Average loss at step  40000 :  5.497142365217209\n",
      "Nearest to b'three': b'five', b'four', b'six', b'two', b'seven', b'eight', b'one', b'nine',\n",
      "Nearest to b'united': b'agave', b'investigation', b'waas', b'agouti', b'micheal', b'airbus', b'following', b'in',\n",
      "Nearest to b's': b'and', b'recitative', b'his', b'akita', b'batll', b'filtering', b'zero', b'agave',\n",
      "Nearest to b'over': b'about', b'acapulco', b'abet', b'disputes', b'abitibi', b'four', b'three', b'in',\n",
      "Nearest to b'for': b'agouti', b'at', b'in', b'with', b'altenberg', b'from', b'of', b'to',\n",
      "Nearest to b'its': b'their', b'the', b'his', b'any', b'a', b'this', b'vdash', b'bos',\n",
      "Nearest to b'more': b'most', b'success', b'thinkpad', b'replica', b'oscillator', b'victoriae', b'bitnet', b'still',\n",
      "Nearest to b'some': b'several', b'many', b'the', b'these', b'android', b'both', b'akita', b'compete',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'implicit', b'replica', b'agouti', b'acetylene', b'till',\n",
      "Nearest to UNK: b'victoriae', b'reuptake', b'dasyprocta', b'vs', b'akita', b'agouti', b'one', b'three',\n",
      "Nearest to b'one': b'two', b'seven', b'four', b'three', b'six', b'five', b'eight', b'reginae',\n",
      "Nearest to b'states': b'aediles', b'ditko', b'mathbf', b'lipids', b'amino', b'exposing', b'heb', b'legionnaires',\n",
      "Nearest to b'may': b'would', b'can', b'will', b'must', b'paraguay', b'could', b'vanuatu', b'eight',\n",
      "Nearest to b'this': b'which', b'it', b'the', b'that', b'reuptake', b'akita', b'a', b'amalthea',\n",
      "Nearest to b'six': b'seven', b'eight', b'four', b'five', b'three', b'nine', b'zero', b'two',\n",
      "Nearest to b'however': b'rc', b'recitative', b'dasyprocta', b'dawn', b'download', b'agouti', b'garand', b'agave',\n",
      "Average loss at step  42000 :  5.301213516354561\n",
      "Average loss at step  44000 :  5.381219238758087\n",
      "Average loss at step  46000 :  5.2972407616376875\n",
      "Average loss at step  48000 :  5.0351949882507325\n",
      "Average loss at step  50000 :  5.145439592957497\n",
      "Nearest to b'three': b'four', b'six', b'five', b'seven', b'two', b'eight', b'one', b'nine',\n",
      "Nearest to b'united': b'investigation', b'agave', b'waas', b'airbus', b'micheal', b'agouti', b'following', b'dramatized',\n",
      "Nearest to b's': b'and', b'his', b'thibetanus', b'zero', b'recitative', b'of', b'two', b'gentiles',\n",
      "Nearest to b'over': b'about', b'acapulco', b'abet', b'in', b'disputes', b'three', b'langevin', b'abitibi',\n",
      "Nearest to b'for': b'agouti', b'in', b'altenberg', b'and', b'with', b'of', b'at', b'from',\n",
      "Nearest to b'its': b'their', b'the', b'his', b'a', b'any', b'this', b'bos', b'her',\n",
      "Nearest to b'more': b'most', b'success', b'searchlight', b'thinkpad', b'positivists', b'mathbf', b'victoriae', b'replica',\n",
      "Nearest to b'some': b'many', b'several', b'these', b'both', b'the', b'various', b'akita', b'android',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'implicit', b'agouti', b'thibetanus', b'replica', b'knuth',\n",
      "Nearest to UNK: b'victoriae', b'thibetanus', b'reuptake', b'agouti', b'dasyprocta', b'abakan', b'akita', b'vs',\n",
      "Nearest to b'one': b'two', b'six', b'eight', b'three', b'four', b'seven', b'five', b'reginae',\n",
      "Nearest to b'states': b'ditko', b'aediles', b'exposing', b'lipids', b'mathbf', b'legionnaires', b'independent', b'heb',\n",
      "Nearest to b'may': b'would', b'can', b'will', b'must', b'could', b'paraguay', b'nine', b'mrna',\n",
      "Nearest to b'this': b'it', b'which', b'the', b'akita', b'reuptake', b'that', b'a', b'amalthea',\n",
      "Nearest to b'six': b'eight', b'four', b'seven', b'three', b'five', b'nine', b'two', b'zero',\n",
      "Nearest to b'however': b'nguni', b'but', b'thibetanus', b'rc', b'that', b'dasyprocta', b'recitative', b'agouti',\n",
      "Average loss at step  52000 :  5.170822853684426\n",
      "Average loss at step  54000 :  5.09776414501667\n",
      "Average loss at step  56000 :  5.058212414383888\n",
      "Average loss at step  58000 :  5.105689479589462\n",
      "Average loss at step  60000 :  4.96149362295866\n",
      "Nearest to b'three': b'five', b'six', b'four', b'two', b'seven', b'eight', b'callithrix', b'one',\n",
      "Nearest to b'united': b'investigation', b'agave', b'callithrix', b'waas', b'airbus', b'michelob', b'agouti', b'following',\n",
      "Nearest to b's': b'tamarin', b'his', b'thibetanus', b'and', b'zero', b'callithrix', b'yoannis', b'filtering',\n",
      "Nearest to b'over': b'about', b'callithrix', b'acapulco', b'in', b'microcebus', b'abet', b'provisioning', b'including',\n",
      "Nearest to b'for': b'agouti', b'of', b'in', b'and', b'ssbn', b'callithrix', b'amalthea', b'altenberg',\n",
      "Nearest to b'its': b'their', b'his', b'the', b'her', b'callithrix', b'any', b'a', b'fisherman',\n",
      "Nearest to b'more': b'most', b'success', b'less', b'tamarin', b'positivists', b'searchlight', b'very', b'thinkpad',\n",
      "Nearest to b'some': b'many', b'several', b'these', b'both', b'the', b'their', b'various', b'all',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'agouti', b'knuth', b'replica', b'recitative', b'implicit',\n",
      "Nearest to UNK: b'tamarin', b'callithrix', b'victoriae', b'reuptake', b'ssbn', b'thibetanus', b'dasyprocta', b'microcebus',\n",
      "Nearest to b'one': b'two', b'six', b'four', b'three', b'five', b'seven', b'eight', b'callithrix',\n",
      "Nearest to b'states': b'aediles', b'ditko', b'exposing', b'lipids', b'independent', b'legionnaires', b'mathbf', b'breeding',\n",
      "Nearest to b'may': b'would', b'can', b'will', b'could', b'must', b'to', b'paraguay', b'should',\n",
      "Nearest to b'this': b'it', b'which', b'the', b'that', b'akita', b'reuptake', b'a', b'amalthea',\n",
      "Nearest to b'six': b'eight', b'four', b'five', b'seven', b'three', b'nine', b'zero', b'callithrix',\n",
      "Nearest to b'however': b'but', b'nguni', b'thibetanus', b'that', b'tamarin', b'rc', b'dasyprocta', b'recitative',\n",
      "Average loss at step  62000 :  4.807210105538368\n",
      "Average loss at step  64000 :  4.7960088357925414\n",
      "Average loss at step  66000 :  4.988517590761185\n",
      "Average loss at step  68000 :  4.924581614494324\n",
      "Average loss at step  70000 :  4.767300604104996\n",
      "Nearest to b'three': b'six', b'five', b'four', b'two', b'seven', b'eight', b'callithrix', b'one',\n",
      "Nearest to b'united': b'investigation', b'callithrix', b'waas', b'following', b'agave', b'airbus', b'michelob', b'agouti',\n",
      "Nearest to b's': b'tamarin', b'thibetanus', b'his', b'thaler', b'gentiles', b'and', b'filtering', b'callithrix',\n",
      "Nearest to b'over': b'about', b'callithrix', b'acapulco', b'microcebus', b'provisioning', b'in', b'abet', b'including',\n",
      "Nearest to b'for': b'in', b'agouti', b'of', b'ssbn', b'amalthea', b'callithrix', b'during', b'to',\n",
      "Nearest to b'its': b'their', b'his', b'the', b'her', b'callithrix', b'any', b'this', b'fisherman',\n",
      "Nearest to b'more': b'most', b'less', b'very', b'success', b'tamarin', b'positivists', b'victoriae', b'lower',\n",
      "Nearest to b'some': b'many', b'several', b'these', b'the', b'both', b'their', b'all', b'various',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'upanija', b'agouti', b'knuth', b'recitative', b'replica',\n",
      "Nearest to UNK: b'callithrix', b'tamarin', b'victoriae', b'dinar', b'reuptake', b'thibetanus', b'microcebus', b'agouti',\n",
      "Nearest to b'one': b'two', b'six', b'four', b'seven', b'three', b'eight', b'five', b'callithrix',\n",
      "Nearest to b'states': b'aediles', b'ditko', b'exposing', b'lipids', b'independent', b'legionnaires', b'mathbf', b'stevens',\n",
      "Nearest to b'may': b'can', b'would', b'will', b'could', b'must', b'should', b'to', b'might',\n",
      "Nearest to b'this': b'it', b'which', b'the', b'that', b'akita', b'reuptake', b'a', b'amalthea',\n",
      "Nearest to b'six': b'eight', b'four', b'five', b'seven', b'three', b'nine', b'two', b'zero',\n",
      "Nearest to b'however': b'but', b'nguni', b'thibetanus', b'that', b'tamarin', b'dasyprocta', b'while', b'callithrix',\n",
      "Average loss at step  72000 :  4.805733936548233\n",
      "Average loss at step  74000 :  4.773093726426363\n",
      "Average loss at step  76000 :  4.867796177327633\n",
      "Average loss at step  78000 :  4.797847882270813\n",
      "Average loss at step  80000 :  4.8162975151538845\n",
      "Nearest to b'three': b'five', b'four', b'six', b'two', b'seven', b'eight', b'one', b'callithrix',\n",
      "Nearest to b'united': b'investigation', b'waas', b'following', b'airbus', b'callithrix', b'agave', b'michelob', b'agouti',\n",
      "Nearest to b's': b'tamarin', b'thibetanus', b'cegep', b'zero', b'callithrix', b'thaler', b'lauder', b'his',\n",
      "Nearest to b'over': b'about', b'acapulco', b'callithrix', b'microcebus', b'clodius', b'four', b'provisioning', b'abet',\n",
      "Nearest to b'for': b'agouti', b'during', b'ssbn', b'callithrix', b'in', b'tamarin', b'altenberg', b'chess',\n",
      "Nearest to b'its': b'their', b'his', b'the', b'her', b'callithrix', b'any', b'cegep', b'severity',\n",
      "Nearest to b'more': b'most', b'less', b'very', b'tamarin', b'positivists', b'success', b'victoriae', b'lower',\n",
      "Nearest to b'some': b'many', b'several', b'these', b'both', b'various', b'the', b'all', b'their',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'upanija', b'agouti', b'recitative', b'victoriae', b'replica',\n",
      "Nearest to UNK: b'tamarin', b'cegep', b'victoriae', b'dinar', b'callithrix', b'reuptake', b'dasyprocta', b'microcebus',\n",
      "Nearest to b'one': b'two', b'six', b'three', b'four', b'seven', b'five', b'microcebus', b'callithrix',\n",
      "Nearest to b'states': b'ditko', b'aediles', b'exposing', b'independent', b'lipids', b'legionnaires', b'stevens', b'breeding',\n",
      "Nearest to b'may': b'can', b'would', b'will', b'could', b'must', b'should', b'might', b'to',\n",
      "Nearest to b'this': b'which', b'it', b'the', b'akita', b'that', b'reuptake', b'what', b'inefficient',\n",
      "Nearest to b'six': b'five', b'four', b'seven', b'eight', b'three', b'nine', b'two', b'zero',\n",
      "Nearest to b'however': b'but', b'that', b'nguni', b'thibetanus', b'tamarin', b'while', b'dasyprocta', b'recitative',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  82000 :  4.790379168272018\n",
      "Average loss at step  84000 :  4.797388158917427\n",
      "Average loss at step  86000 :  4.773596536278725\n",
      "Average loss at step  88000 :  4.696155305504799\n",
      "Average loss at step  90000 :  4.760957259654999\n",
      "Nearest to b'three': b'five', b'two', b'four', b'six', b'seven', b'eight', b'callithrix', b'one',\n",
      "Nearest to b'united': b'investigation', b'airbus', b'waas', b'following', b'callithrix', b'agave', b'michelob', b'thaler',\n",
      "Nearest to b's': b'tamarin', b'his', b'thibetanus', b'cegep', b'callithrix', b'dinar', b'zero', b'filtering',\n",
      "Nearest to b'over': b'about', b'microcebus', b'acapulco', b'provisioning', b'callithrix', b'clodius', b'adoptive', b'abet',\n",
      "Nearest to b'for': b'agouti', b'of', b'during', b'ssbn', b'callithrix', b'when', b'tamarin', b'altenberg',\n",
      "Nearest to b'its': b'their', b'his', b'the', b'her', b'callithrix', b'stenella', b'any', b'some',\n",
      "Nearest to b'more': b'most', b'less', b'very', b'tamarin', b'positivists', b'success', b'victoriae', b'thinkpad',\n",
      "Nearest to b'some': b'many', b'several', b'these', b'both', b'all', b'various', b'most', b'the',\n",
      "Nearest to b'so': b'beck', b'proper', b'hitler', b'upanija', b'picnics', b'knuth', b'agouti', b'recitative',\n",
      "Nearest to UNK: b'tamarin', b'callithrix', b'victoriae', b'cegep', b'reuptake', b'thibetanus', b'thaler', b'dasyprocta',\n",
      "Nearest to b'one': b'two', b'four', b'seven', b'six', b'three', b'five', b'tamarin', b'callithrix',\n",
      "Nearest to b'states': b'aediles', b'ditko', b'exposing', b'lipids', b'independent', b'legionnaires', b'stevens', b'fsm',\n",
      "Nearest to b'may': b'can', b'would', b'will', b'could', b'must', b'might', b'should', b'nine',\n",
      "Nearest to b'this': b'it', b'which', b'the', b'akita', b'that', b'reuptake', b'inefficient', b'what',\n",
      "Nearest to b'six': b'seven', b'five', b'eight', b'four', b'three', b'nine', b'two', b'callithrix',\n",
      "Nearest to b'however': b'but', b'that', b'nguni', b'thibetanus', b'tamarin', b'while', b'dasyprocta', b'callithrix',\n",
      "Average loss at step  92000 :  4.7134491041898725\n",
      "Average loss at step  94000 :  4.627248567819596\n",
      "Average loss at step  96000 :  4.718428887248039\n",
      "Average loss at step  98000 :  4.611153920173645\n",
      "Average loss at step  100000 :  4.66572683262825\n",
      "Nearest to b'three': b'five', b'four', b'six', b'two', b'seven', b'eight', b'callithrix', b'one',\n",
      "Nearest to b'united': b'mtsho', b'airbus', b'following', b'waas', b'investigation', b'callithrix', b'michelob', b'agave',\n",
      "Nearest to b's': b'his', b'tamarin', b'thibetanus', b'callithrix', b'the', b'cegep', b'ssbn', b'and',\n",
      "Nearest to b'over': b'about', b'microcebus', b'provisioning', b'three', b'callithrix', b'acapulco', b'clodius', b'four',\n",
      "Nearest to b'for': b'agouti', b'callithrix', b'ssbn', b'when', b'during', b'tamarin', b'with', b'microcebus',\n",
      "Nearest to b'its': b'their', b'his', b'the', b'her', b'callithrix', b'stenella', b'accountant', b'tragedians',\n",
      "Nearest to b'more': b'most', b'less', b'very', b'tamarin', b'positivists', b'success', b'lower', b'victoriae',\n",
      "Nearest to b'some': b'many', b'several', b'these', b'both', b'all', b'various', b'the', b'their',\n",
      "Nearest to b'so': b'beck', b'proper', b'deoxyribose', b'hitler', b'upanija', b'picnics', UNK, b'knuth',\n",
      "Nearest to UNK: b'tamarin', b'callithrix', b'victoriae', b'cegep', b'microcebus', b'reuptake', b'dinar', b'ssbn',\n",
      "Nearest to b'one': b'two', b'six', b'four', b'three', b'seven', b'five', b'callithrix', b'microcebus',\n",
      "Nearest to b'states': b'aediles', b'ditko', b'legionnaires', b'exposing', b'lipids', b'independent', b'stevens', b'fsm',\n",
      "Nearest to b'may': b'can', b'would', b'will', b'could', b'must', b'might', b'should', b'to',\n",
      "Nearest to b'this': b'it', b'which', b'the', b'akita', b'that', b'reuptake', b'what', b'each',\n",
      "Nearest to b'six': b'seven', b'five', b'eight', b'four', b'nine', b'three', b'two', b'callithrix',\n",
      "Nearest to b'however': b'but', b'that', b'while', b'nguni', b'tamarin', b'thibetanus', b'and', b'dasyprocta',\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: 필요한 데이터를 다운로드한다.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"파일이 존재하지 않으면 다운로드하고 사이즈가 적절한지 체크한다.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "# 문자열로 데이터를 읽는다\n",
    "def read_data(filename):\n",
    "  \"\"\"zip파일 압축을 해제하고 단어들의 리스트를 읽는다.\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = f.read(f.namelist()[0]).split()\n",
    "  return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))\n",
    "\n",
    "# Step 2: dictionary를 만들고 UNK 토큰을 이용해서 rare words를 교체(replace)한다.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "# Step 3: skip-gram model을 위한 트레이닝 데이터(batch)를 생성하기 위한 함수.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "      '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: skip-gram model 만들고 학습시킨다.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # embedding vector의 크기.\n",
    "skip_window = 1       # 윈도우 크기 : 왼쪽과 오른쪽으로 얼마나 많은 단어를 고려할지를 결정.\n",
    "num_skips = 2         # 레이블(label)을 생성하기 위해 인풋을 얼마나 많이 재사용 할 것인지를 결정.\n",
    "\n",
    "# sample에 대한 validation set은 원래 랜덤하게 선택해야한다. 하지만 여기서는 validation samples을 \n",
    "# 가장 자주 생성되고 낮은 숫자의 ID를 가진 단어로 제한한다.\n",
    "valid_size = 16     # validation 사이즈.\n",
    "valid_window = 100  # 분포의 앞부분(head of the distribution)에서만 validation sample을 선택한다.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # sample에 대한 negative examples의 개수.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # 트레이닝을 위한 인풋 데이터들\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # embedding vectors 행렬을 랜덤값으로 초기화\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # 행렬에 트레이닝 데이터를 지정\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # NCE loss를 위한 변수들을 선언\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # batch의 average NCE loss를 계산한다.\n",
    "  # tf.nce_loss 함수는 loss를 평가(evaluate)할 때마다 negative labels을 가진 새로운 샘플을 자동적으로 생성한다.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # SGD optimizer를 생성한다.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # minibatch examples과 모든 embeddings에 대해 cosine similarity를 계산한다.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Step 5: 트레이닝을 시작한다.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # 트레이닝을 시작하기 전에 모든 변수들을 초기화한다.\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "    # optimizer op을 평가(evaluating)하면서 한 스텝 업데이트를 진행한다.\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # 평균 손실(average loss)은 지난 2000 배치의 손실(loss)로부터 측정된다.\n",
    "      print(\"Average loss at step \", step, \": \", average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # nearest neighbors의 개수\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log_str = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = \"%s %s,\" % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# Step 6: embeddings을 시각화한다.\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "  plt.figure(figsize=(18, 18))  #in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i,:]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "  print(\"Please install sklearn and matplotlib to visualize embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
