{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [0., 3., 5.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DictVectorizer는 사전 형태로 되어 있는 feature 정보를 matrix 형태로 변환하기 위한 것\n",
    "v = DictVectorizer(sparse = False)\n",
    "D = [{'A':1, 'B':2}, {'B':3, 'C':5}]\n",
    "X = v.fit_transform(D)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A': 1.0, 'B': 2.0}, {'B': 3.0, 'C': 5.0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'C':4, 'D':3 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "CountVectorizer는 다양한 인수를 가진다. 그 중 중요한 것들은 다음과 같다.\n",
    "\n",
    "- stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "- stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "- analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "- tokenizer : 함수 또는 None (디폴트)\n",
    "토큰 생성 함수 .\n",
    "- token_pattern : string\n",
    "토큰 정의용 정규 표현식\n",
    "- ngram_range : (min_n, max_n) 튜플\n",
    "n-그램 범위\n",
    "- max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최대 빈도\n",
    "- min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최소 빈도\n",
    "- vocabulary : 사전이나 리스트\n",
    "단어장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'this is the second document',\n",
    "    'and the third'\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'second': 4,\n",
       " 'the': 5,\n",
       " 'third': 6,\n",
       " 'this': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "- Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. 보통 영어의 **관사나 접속사, 한국어의 조사** 등이 여기에 해당한다. stop_words 인수로 조절할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0,\n",
       " 'first': 1,\n",
       " 'is': 2,\n",
       " 'second': 3,\n",
       " 'the': 4,\n",
       " 'third': 5,\n",
       " 'this': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "- analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'a': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'm': 9,\n",
       " 'n': 10,\n",
       " 'o': 11,\n",
       " 'r': 12,\n",
       " 's': 13,\n",
       " 't': 14,\n",
       " 'u': 15}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adonishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'and': 1,\n",
       " 'document': 2,\n",
       " 'first': 3,\n",
       " 'is': 4,\n",
       " 'second': 5,\n",
       " 'the': 6,\n",
       " 'third': 7,\n",
       " 'this': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 알파벳 순서순 word_tokenizing.\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "vect = CountVectorizer(tokenizer = nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "- n-그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. 1-그램은 토큰 하나만 단어로 사용하며 2-그램은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'second document': 3,\n",
       " 'the first': 4,\n",
       " 'the second': 5,\n",
       " 'the third': 6,\n",
       " 'this is': 7}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'is': 1, 'the': 2, 'this': 3}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수\n",
    "# max_df, min_df 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. \n",
    "# 토큰의 빈도가 max_df로 지정한 값을 초과 하거나 min_df로 지정한 값보다 작은 경우에는 무시한다. \n",
    "# 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다\n",
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and', 'first', 'second', 'third'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩\n",
    "# 공통적으로 들어있는 단어의 가중치를 축소\n",
    "# tf(d,t) : 단어의 빈도수\n",
    "# idf(d,t): inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43306685, 0.56943086, 0.43306685, 0.        ,\n",
       "        0.33631504, 0.        , 0.43306685],\n",
       "       [0.        , 0.43306685, 0.        , 0.43306685, 0.56943086,\n",
       "        0.33631504, 0.        , 0.43306685],\n",
       "       [0.65249088, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38537163, 0.65249088, 0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hashing Trick\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashing Vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.34 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "- using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "import nltk\n",
    "hannanum = Hannanum()\n",
    "#ipython file\n",
    "f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에서는 하나의 문서가 하나의 단어로만 이루어져 있다. \n",
    "\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [\n",
    "    w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x271024e92e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#백터의합으로 빈도 가시화 ->\n",
    "# CountVectorizer로 이 문서 집합을 처리하면 각 문서는 하나의 원소만 1이고 나머지 원소는 0인 벡터가 된다. \n",
    "# 이 벡터의 합으로 빈도를 알아보았다.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis = 0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
